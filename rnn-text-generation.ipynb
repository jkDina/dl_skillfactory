{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "## Get the Shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 1115394 characters\n",
      "=====================================\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_fpath = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', \n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "text = codecs.open(data_fpath, 'r', encoding='utf8').read()\n",
    "\n",
    "print('Length of the text: {} characters'.format(len(text)))\n",
    "print('=====================================')\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "source": [
    "## Extract the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "print ('{} unique characters'.format(VOCAB_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "## Create char2idx / idx2char dictionaries and convert the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n':   0,\n",
      "' ' :   1,\n",
      "'!' :   2,\n",
      "'$' :   3,\n",
      "'&' :   4,\n",
      "\"'\" :   5,\n",
      "',' :   6,\n",
      "'-' :   7,\n",
      "'.' :   8,\n",
      "'3' :   9,\n",
      "':' :  10,\n",
      "';' :  11,\n",
      "'?' :  12,\n",
      "'A' :  13,\n",
      "'B' :  14,\n",
      "'C' :  15,\n",
      "'D' :  16,\n",
      "'E' :  17,\n",
      "'F' :  18,\n",
      "'G' :  19,\n",
      "...\n",
      "=====================================\n",
      "Example of the encoded text: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "for char, _ in zip(char2idx, range(20)):\n",
    "    print('{:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('...')\n",
    "print('=====================================')\n",
    "print('Example of the encoded text: {}'.format(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare TF data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "examples_per_epoch = len(text) // SEQ_LEN\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "dataset = dataset.batch(SEQ_LEN+1, drop_remainder=True)\n",
    "dataset = dataset.map(split_input_target)\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "## Take an example from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "=====================================\n",
      "Input data:  \" as you are weary of the weight,\\nRest you, whiles I lament King Henry's corse.\\n\\nGLOUCESTER:\\nStay, yo\"\n",
      "Target data: \"as you are weary of the weight,\\nRest you, whiles I lament King Henry's corse.\\n\\nGLOUCESTER:\\nStay, you\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example indataset.take(1):\n",
    "    print('=====================================')\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()[0]])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(batch_size):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.CuDNNGRU(1024,\n",
    "                                 return_sequences=True,\n",
    "                                 recurrent_initializer='glorot_uniform',\n",
    "                                 stateful=True),\n",
    "        tf.keras.layers.Dense(VOCAB_SIZE),\n",
    "    ])\n",
    "\n",
    "model = build_model(batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "![A drawing of the data passing through the model](https://tensorflow.org/tutorials/sequences/images/text_generation_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(batch_size, sequence_length, vocab_size) : (64, 100, 65)\n",
      "=====================================\n",
      "Input: \n",
      " 'sea\\nWhose envious gulf did swallow up his life.\\nAh, kill me with thy weapon, not with words!\\nMy brea'\n",
      "=====================================\n",
      "Next Char Predictions: \n",
      " \"cpR?!u;B'RSrd?&LBq hovm.;nd&sKRyqYBJDVoptRC'\\nnEXqjHr:,WhlrsJxWclFRmh dQyOlq!un?fDP?lWQhs3GY!,iqwBvcS\"\n"
     ]
    }
   ],
   "source": [
    "input_example_batch, target_example_batch = list(dataset.take(1))[0]\n",
    "example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "\n",
    "print(\"(batch_size, sequence_length, vocab_size) : {}\".format(example_batch_predictions.shape))\n",
    "print('=====================================')\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print('=====================================')\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Define loss and prepare for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss=loss)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "## Run the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "173/174 [============================>.] - ETA: 0s - loss: 2.6891WARNING:tensorflow:From C:\\Users\\LK\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n",
      "174/174 [==============================] - 21s 122ms/step - loss: 2.6860\n",
      "Epoch 2/10\n",
      "174/174 [==============================] - 19s 112ms/step - loss: 1.9536\n",
      "Epoch 3/10\n",
      "174/174 [==============================] - 20s 113ms/step - loss: 1.6873\n",
      "Epoch 4/10\n",
      "174/174 [==============================] - 20s 114ms/step - loss: 1.5401\n",
      "Epoch 5/10\n",
      "174/174 [==============================] - 20s 116ms/step - loss: 1.4513\n",
      "Epoch 6/10\n",
      "174/174 [==============================] - 22s 124ms/step - loss: 1.3922\n",
      "Epoch 7/10\n",
      "174/174 [==============================] - 22s 128ms/step - loss: 1.3470\n",
      "Epoch 8/10\n",
      "174/174 [==============================] - 22s 128ms/step - loss: 1.3073\n",
      "Epoch 9/10\n",
      "174/174 [==============================] - 22s 128ms/step - loss: 1.2746\n",
      "Epoch 10/10\n",
      "174/174 [==============================] - 23s 129ms/step - loss: 1.2424\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(dataset.repeat(), \n",
    "                    epochs=EPOCHS, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "## Restore the latest checkpoint and rebuild the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "source": [
    "## Define a function for the text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate):\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![To generate text the model's output is fed back to the input](https://tensorflow.org/tutorials/sequences/images/text_generation_sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "We'll keep his wishARDICHARD III:\n",
      "Exetch, now I prithee now;\n",
      "If he's call him, let us he present her my hearts,\n",
      "But only they shall bring white vantage of\n",
      "her in laty of windol seas.\n",
      "\n",
      "KING RICHARD III:\n",
      "I do hell out yourselv stir it better:\n",
      "'Tis my short mis-shame! thou dery with that.\n",
      "\n",
      "BAPTISTA:\n",
      "Why, vow, by these gentletoran mortal, poison whose sweet,\n",
      "To take the last dignior plumage\n",
      "O, thereto be you in dine highest.\n",
      "\n",
      "First Soldier:\n",
      "How in honry, good my lord.\n",
      "You'll I were the act dost rlack.\n",
      "\n",
      "ROMEO:\n",
      "I fear, as you arry, as if\n",
      "A better every stop-well desire of herwity.\n",
      "\n",
      "ROMEO:\n",
      "I'll have gurshep'd, there hath intend it well.\n",
      "'Mourned life?\n",
      "\n",
      "KING EDWARD IV:\n",
      "But look up to strong horses. Getty is thou\n",
      "As I with me, Kate! O, that\n",
      "your hard-breath, but to my gilt of fiery!\n",
      "Without draw of darlike;\n",
      "At id a balk pricely of their cest:\n",
      "Under the very brother I did but branch you\n",
      "Both harroto thy rights at nife to this\n",
      "within the prince at the bastard fine scept of the morning, so drink \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO:\", num_generate=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
