{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"rnn-text-generation.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"yG_n40gFzf9s","colab":{}},"source":["import codecs\n","import numpy as np\n","import os\n","import time\n","\n","import tensorflow as tf\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EHDoRoc5PKWz"},"source":["## Get the Shakespeare dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pD_55cOxLkAb","colab":{}},"source":["data_fpath = tf.keras.utils.get_file(\n","    'shakespeare.txt', \n","    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n","\n","text = codecs.open(data_fpath, 'r', encoding='utf8').read()\n","\n","print('Length of the text: {} characters'.format(len(text)))\n","print('=====================================')\n","print(text[:250])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Duhg9NrUymwO","colab":{}},"source":["## Extract the vocabulary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IlCgQBRVymwR","colab":{}},"source":["vocab = sorted(set(text))\n","VOCAB_SIZE = len(vocab)\n","\n","print ('{} unique characters'.format(VOCAB_SIZE))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LFjSVAlWzf-N"},"source":["## Create char2idx / idx2char dictionaries and convert the text"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IalZLbvOzf-F","colab":{}},"source":["char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","for char, _ in zip(char2idx, range(20)):\n","    print('{:4s}: {:3d},'.format(repr(char), char2idx[char]))\n","print('...')\n","print('=====================================')\n","print('Example of the encoded text: {}'.format(text_as_int[:13]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chopt4bw3z0z","colab_type":"text"},"source":["## Prepare TF data pipeline"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0UHJDA39zf-O","colab":{}},"source":["SEQ_LEN = 100\n","BATCH_SIZE = 64\n","\n","examples_per_epoch = len(text) // SEQ_LEN\n","steps_per_epoch = examples_per_epoch // BATCH_SIZE\n","\n","def split_input_target(chunk):\n","    input_text = chunk[:-1]\n","    target_text = chunk[1:]\n","    return input_text, target_text\n","\n","dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","dataset = dataset.batch(SEQ_LEN+1, drop_remainder=True)\n","dataset = dataset.map(split_input_target)\n","\n","dataset = dataset.shuffle(buffer_size=10000)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-ZSYAcQV8OGP"},"source":["## Take an example from the dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9NGu-FkO_kYU","colab":{}},"source":["for input_example, target_example in dataset.take(1):\n","    print('=====================================')\n","    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()[1]])))\n","    print ('Target data:', repr(''.join(idx2char[target_example.numpy()[1]])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r6oUuElIMgVx"},"source":["## Build the model"]},{"cell_type":"code","metadata":{"id":"DPIKjLAW3z0_","colab_type":"code","colab":{}},"source":["def build_model(batch_size):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Embedding(VOCAB_SIZE, 256, batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.CuDNNGRU(1024,\n","                                 return_sequences=True,\n","                                 recurrent_initializer='glorot_uniform',\n","                                 stateful=True),\n","        tf.keras.layers.Dense(VOCAB_SIZE),\n","    ])\n","\n","model = build_model(batch_size=BATCH_SIZE)\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RkA5upJIJ7W7"},"source":["![A drawing of the data passing through the model](https://tensorflow.org/tutorials/sequences/images/text_generation_training.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-ubPo0_9Prjb"},"source":["## Try the model before training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C-_70kKAPrPU","colab":{}},"source":["input_example_batch, target_example_batch = list(dataset.take(1))[0]\n","example_batch_predictions = model(input_example_batch)\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n","\n","print(\"(batch_size, sequence_length, vocab_size) : {}\".format(example_batch_predictions.shape))\n","print('=====================================')\n","print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n","print('=====================================')\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LJL0Q0YPY6Ee"},"source":["## Define loss and prepare for the training"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DDl1_Een6rL0","colab":{}},"source":["def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","model.compile(optimizer=tf.train.AdamOptimizer(),\n","              loss=loss)\n","\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Ky3F_BhgkTW"},"source":["## Run the training procedure"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UK-hmKjYVoll","colab":{}},"source":["EPOCHS = 10\n","\n","history = model.fit(dataset.repeat(), \n","                    epochs=EPOCHS, \n","                    steps_per_epoch=steps_per_epoch, \n","                    callbacks=[checkpoint_callback])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JIPcXllKjkdr"},"source":["## Restore the latest checkpoint and rebuild the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LycQ-ot_jjyu","colab":{}},"source":["model = build_model(batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"71xa6jnYVrAN","colab":{}},"source":["## Define a function for the text generation"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WvuwZBX5Ogfd","colab":{}},"source":["def generate_text(model, start_string, num_generate):\n","\n","    input_eval = [char2idx[s] for s in start_string]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    text_generated = []\n","\n","    # Low temperatures results in more predictable text.\n","    # Higher temperatures results in more surprising text.\n","    temperature = 1.0\n","\n","    model.reset_states()\n","    for i in range(num_generate):\n","        predictions = model(input_eval)\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # Using a multinomial distribution to predict the word returned by the model\n","        predictions = predictions / temperature\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return (start_string + ''.join(text_generated))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILfalqqg3z1a","colab_type":"text"},"source":["![To generate text the model's output is fed back to the input](https://tensorflow.org/tutorials/sequences/images/text_generation_sampling.png)"]},{"cell_type":"markdown","metadata":{"id":"HxxvizFA3z1a","colab_type":"text"},"source":["## Generate the text"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ktovv0RFhrkn","colab":{}},"source":["print(generate_text(model, start_string=u\"ROMEO:\", num_generate=1000))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GR2-8kW63z1e","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}